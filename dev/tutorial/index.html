<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · Chairmarks.jl</title><meta name="title" content="Tutorial · Chairmarks.jl"/><meta property="og:title" content="Tutorial · Chairmarks.jl"/><meta property="twitter:title" content="Tutorial · Chairmarks.jl"/><meta name="description" content="Documentation for Chairmarks.jl."/><meta property="og:description" content="Documentation for Chairmarks.jl."/><meta property="twitter:description" content="Documentation for Chairmarks.jl."/><meta property="og:url" content="https://Chairmarks.lilithhafner.com/tutorial/"/><meta property="twitter:url" content="https://Chairmarks.lilithhafner.com/tutorial/"/><link rel="canonical" href="https://Chairmarks.lilithhafner.com/tutorial/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Chairmarks.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../why/">Why use Chairmarks?</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Common-pitfalls"><span>Common pitfalls</span></a></li><li><a class="tocitem" href="#Running-many-benchmarks"><span>Running many benchmarks</span></a></li><li><a class="tocitem" href="#Advanced-usage"><span>Advanced usage</span></a></li></ul></li><li><span class="tocitem">How To</span><ul><li><a class="tocitem" href="../migration/">...migrate from BenchmarkTools</a></li><li><a class="tocitem" href="../autoload/">...install Charimarks ergonomically</a></li><li><a class="tocitem" href="../regressions/">...perform automated regression testing on a package</a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/LilithHafner/Chairmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/LilithHafner/Chairmarks.jl/blob/main/docs/src/tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial"><a class="docs-heading-anchor" href="#Tutorial">Tutorial</a><a id="Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Tutorial" title="Permalink"></a></h1><p>Welcome! This tutorial assumes very little prior knowledge and walks you through how to become a competent user of Chairmarks. If you are already an experienced user of BenchmarkTools, you may want to read about <a href="../migration/#migration">how to migrate from BenchmarkTools to Chairmarks</a> instead.</p><p>If you don&#39;t have Julia already, download it from <a href="https://julialang.org/downloads/">julialang.org/downloads</a>.</p><p>Now, launch a Julia REPL by typing <code>julia</code> at the command line.</p><p>To install Chairmarks, type <code>]</code> to enter the package manager, and then type</p><pre><code class="language-julia-repl hljs">(@v1.xx) pkg&gt; add Chairmarks</code></pre><p>This will install Chairmarks into your default environment. Unlike most packages, installing Chairmarks into your default environment <em>is</em> recommended because it is a very lightweight package and a development tool.</p><p>Now, you can use Chairmarks by typing <code>using Chairmarks</code> in the REPL. Press backspace to exit the package manager and return to the REPL and run</p><pre><code class="language-julia-repl hljs">julia&gt; using Chairmarks

julia&gt; @b rand(100)
95.500 ns (2 allocs: 928 bytes)</code></pre><p>Congratulations! This is your first result from Chairmarks. Let&#39;s look a little closer at the invocation and results. <code>@b</code> is a macro exported from Chairmarks. It takes the expression <code>rand(100)</code> and runs it a bunch of times, measuring how long it takes to run.</p><p>The result, <code>95.500 ns (2 allocs: 928 bytes)</code> tells us that the expression takes 95.5 nanoseconds to run and allocates 928 bytes of memory spread across two distinct allocation events. The exact results you get will likely differ based on your hardware and the Julia version you are using. These results from Julia 1.11.</p><p>Chairmarks reports results in seconds (s), milliseconds (ms), microseconds (μs), or nanoseconds (ns) depending on the magnitude of the runtime. Each of these units is 1000 times smaller than the last according to the standard <a href="https://en.wikipedia.org/wiki/Metric_prefix">SI unit system</a>.</p><p>By default, Chairmarks reports the <em>fastest</em> runtime of the expression. This is typically the best choice for reducing noise in microbenchmarks as things like garbage collection and other background tasks can cause inconsistent slowdowns but but speedups. If you want to get the full results, use the <code>@be</code> macro. (<code>@be</code> is longer than <code>@b</code> and gives a longer output)</p><pre><code class="language-julia-repl hljs">julia&gt; @be rand(100)
Benchmark: 19442 samples with 25 evaluations
min    95.000 ns (2 allocs: 928 bytes)
median 103.320 ns (2 allocs: 928 bytes)
mean   140.096 ns (2 allocs: 928 bytes, 0.36% gc time)
max    19.748 μs (2 allocs: 928 bytes, 96.95% gc time)</code></pre><p>This invocation runs the same experiment as <code>@b</code>, but reports more results. It ran 19442 samples, each of which involved recording some performance counters, running <code>rand(100)</code> 25 times, and then recording the performance counters again and computing the difference. The reported runtimes and allocations are those differences divided by the number of evaluations. We can see here that the runtime of <code>rand(100)</code> is pretty stable. 50% of the time it ranges between 95 and 103.3 nanoseconds. However, the maximum time is two orders of magnitude slower than the mean time. This is because the maximum time includes a garbage collection event that took 96.95% of the time.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><p>Sometimes, we wish to measure the runtime of a function that requires some data to operate on, but don&#39;t want to measure the runtime of the function that generates the data. For example, we may want to compare how long it takes to hash an array of numbers, but we don&#39;t want to include the time it takes to generate the input in our measurements. We can do this using Chairmarks&#39; pipeline syntax:</p><pre><code class="language-julia-repl hljs">julia&gt; @b rand(100) hash
166.665 ns</code></pre><p>The first argument is called once per sample, and the second argument is called once per evaluation, each time passed the result of the first argument. We can also use the special <code>_</code> variable to refer to the output of the previous step. Here, we compare two different implementations of the norm of a vector</p><pre><code class="language-julia-repl hljs">julia&gt; @b rand(100) sqrt(sum(_ .* _))
37.628 ns (2 allocs: 928 bytes)

julia&gt; @b rand(100) sqrt(sum(x-&gt;x^2, _))
11.053 ns</code></pre><p>The _ refers to the array whose norm is to be computed. Both implementations are quite fast. These measurements are on a 3.5 GHz CPU so it appears that the first implementation takes about one clock cycle per element, with a bit of overhead. The second, on the other hand, appears to be running much faster than that, likely because it is making use of SIMD instructions.</p><h2 id="Common-pitfalls"><a class="docs-heading-anchor" href="#Common-pitfalls">Common pitfalls</a><a id="Common-pitfalls-1"></a><a class="docs-heading-anchor-permalink" href="#Common-pitfalls" title="Permalink"></a></h2><p>When benchmarking a function which mutates its arguments, be aware that the same input is passed to the function each evaluation in a sample. This can cause problems if the function does not expect to repeatedly operate on the same input.</p><pre><code class="language-julia-repl hljs">julia&gt; @b rand(100) sort!
129.573 ns (0.02 allocs: 11.317 bytes)</code></pre><p>We can see immediately that something suspicious is going on here: the reported number of allocations (which we expect to be an integer) is a floating point number. This is because each sample, the array is sorted once, which involves allocating a scratchspace, and then that same array is re-sorted repeatedly. It turns out <code>sort!</code> operates very quickly and does not allocate at all when it is passed a sorted array. To benchmark this more accurately, we may specify the number of evaluations</p><pre><code class="language-julia-repl hljs">julia&gt; @b rand(100) sort! evals=1
1.208 μs (2 allocs: 928 bytes)</code></pre><p>or copy the input before sorting it</p><pre><code class="language-julia-repl hljs">julia&gt; @b rand(100) sort!(copy(_))
1.250 μs (4 allocs: 1.812 KiB)</code></pre><p>copy the input into a pre-allocated array</p><pre><code class="language-julia-repl hljs">julia&gt; @b (x = rand(100); (x, copy(x))) sort!(copyto!(_[1], _[2]))
675.926 ns (2 allocs: 928 bytes)</code></pre><p>or re-generate the input each evaluation</p><pre><code class="language-julia-repl hljs">julia&gt; @b sort!(rand(100))
1.405 μs (4 allocs: 1.812 KiB)</code></pre><p>Notice that each of these invocations produces a different output. Setting <code>evals</code> to 1 can cause strange effects whenever the runtime of the expression is less than about 30 μs both due to the overhead of starting and stopping the timers and due to the imprecision of timer results on most machines. Any form of pre-processing included in the primary function will be included in the reported runtime, so each of the latter options also introduce artifacts.</p><p>In general, it is important to use the same methodology when comparing two different functions. Chairmarks is optimized to produce reliable results for answering questions of the form &quot;which of these two implementations of the same specification is faster&quot;, more so than providing absolute measurements of the runtime of fast-running functions.</p><p>That said, for functions which take more than about 30 μs to run, Chairmarks can reliably provide accurate absolute timings. In general, the faster the runtime of the expression being measured, the more strange behavior and artifacts you will see, and the more careful you have to be.</p><pre><code class="language-julia-repl hljs">julia&gt; f() = sum(rand(100_000))
f (generic function with 1 method)

julia&gt; @b f()
67.167 μs (3 allocs: 781.312 KiB)

julia&gt; @b f() evals=1
67.334 μs (3 allocs: 781.312 KiB)

julia&gt; @b for _ in 1:3 f() end
201.917 μs (9 allocs: 2.289 MiB)

julia&gt; 201.917/67.167
3.0061935176500363

julia&gt; 201.917/67.334
2.998737636261027</code></pre><p>Longer runtimes and macrobenchmarks are much more trustworthy than microbenchmarks, though microbenchmarks are often a great tool for identifying performance bottlenecks and optimizing macrobenchmarks.</p><h2 id="Running-many-benchmarks"><a class="docs-heading-anchor" href="#Running-many-benchmarks">Running many benchmarks</a><a id="Running-many-benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Running-many-benchmarks" title="Permalink"></a></h2><p>It&#39;s pretty straightforward to benchmark a whole parameter sweep to check performance figures. Just invoke <code>@b</code> or <code>@be</code> repeatedly. For example, if you want to know how allocation times vary with input size, you could run this list comprehension which runs <code>@b fill(0, n)</code> for each power of 4 from 4 to 4^10:</p><pre><code class="language-julia-repl hljs">julia&gt; [@b fill(0, n) for n in 4 .^ (1:10)]
10-element Vector{Chairmarks.Sample}:
 9.752 ns (2 allocs: 96 bytes)
 11.040 ns (2 allocs: 192 bytes)
 27.859 ns (2 allocs: 576 bytes)
 128.009 ns (3 allocs: 2.062 KiB)
 122.513 ns (3 allocs: 8.062 KiB)
 346.962 ns (3 allocs: 32.062 KiB)
 1.055 μs (3 allocs: 128.062 KiB)
 3.597 μs (3 allocs: 512.062 KiB)
 11.417 μs (3 allocs: 2.000 MiB)
 88.084 μs (3 allocs: 8.000 MiB)</code></pre><p>The default runtime of a benchmark is 0.1 seconds, so this invocation should take just over 1 second to run. Let&#39;s verify:</p><pre><code class="language-julia-repl hljs">julia&gt; @time [@b fill(0, n) for n in 4 .^ (1:10)];
  1.038502 seconds (27.16 M allocations: 22.065 GiB, 27.03% gc time, 3.59% compilation time)</code></pre><p>If we want a wider parameter sweep, we can use the <code>seconds</code> parameter to configure how long benchmarking will take. However, once we start setting seconds to a value below <code>0.1</code>, the benchmarking itself becomes performance sensitive and, from <a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Performance-critical-code-should-be-inside-a-function">the performance tips</a>, performance critical code should be inside a function. So we should put the call to <code>@b</code> or <code>@be</code> into a function.</p><pre><code class="language-julia-repl hljs">julia&gt; f(n, t) = @b fill(0, n) seconds=t
f (generic function with 1 method)

julia&gt; @time f.(1:1000, .001)
  1.089171 seconds (20.88 M allocations: 18.901 GiB, 19.87% gc time, 1.81% compilation time)
1000-element Vector{Chairmarks.Sample}:
 10.286 ns (2 allocs: 64 bytes)
 10.628 ns (2 allocs: 80 bytes)
 10.607 ns (2 allocs: 80 bytes)
 10.723 ns (2 allocs: 96 bytes)
 ⋮
 129.294 ns (3 allocs: 7.875 KiB)
 129.294 ns (3 allocs: 7.875 KiB)
 129.471 ns (3 allocs: 7.875 KiB)
 130.570 ns (3 allocs: 7.875 KiB)</code></pre><p>Setting the <code>seconds</code> parameter too low can cause benchmarks to be noisy. It&#39;s good practice to run a benchmark at least a couple of times no matter what the configuration is to make sure it&#39;s reasonably stable.</p><h2 id="Advanced-usage"><a class="docs-heading-anchor" href="#Advanced-usage">Advanced usage</a><a id="Advanced-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-usage" title="Permalink"></a></h2><p>It is possible to manually specify the number of evaluations, samples, and/or seconds to run benchmarking for. It is also possible to pass a teardown function or an initialization function that runs only once. See the docstring of <a href="../reference/#Chairmarks.@be-Tuple"><code>@be</code></a> for more information on these additional arguments.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>note that the samples are aggregated element wise, so the max field reports the maximum runtime and the maximum proportion of runtime spent in garbage collection (gc). Thus it is possible that the trial which had a 19.748 μs runtime was not the same trial that spent 96.95% of its time in garbage collection. This is in order to make the results more consistent. If half the trials spend 10% of their time in gc amd runtime varies based on other factors, it would be unfortunate to report maximum gc time as either 10% or 0% at random depending on whether the longest running trial happened to trigger gc.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../why/">« Why use Chairmarks?</a><a class="docs-footer-nextpage" href="../migration/">...migrate from BenchmarkTools »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Sunday 3 March 2024 15:06">Sunday 3 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
